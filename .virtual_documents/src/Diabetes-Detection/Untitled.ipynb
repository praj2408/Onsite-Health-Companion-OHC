


# import the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix
import joblib



# read the dataset
data = pd.read_csv("diabetes.csv")


## read the first few and last few data
data.head()


data.tail()


data.info()


## describe the statistics
data.describe().T


data.isna().sum()


# visualize the missing values for sample of 200
plt.figure(figsize = (18,12))
msno.matrix(data.sample(200))


plt.figure(figsize = (18,12))
sns.heatmap(data.isna(),cmap='BrBG')





#### check skewness and kurtosis
data.skew()


data.kurt()


sns.pairplot(data = data)


correlation = data.corr()
plt.figure(figsize = (18,12))
sns.heatmap(correlation, cmap = 'BrBG', annot = True)


corr_report = data.corr()['Outcome']
print(corr_report)





# create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data.items():
    col_dist = sns.histplot(value, ax=ax[index], color='blue',kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)





data.head()


# create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data.items():
    col_dist = sns.boxplot(value, ax=ax[index])
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)


Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
print(IQR)





# remove the outlier
data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis = 1)]
data.shape


sns.boxplot(data["Pregnancies"])


sns.boxplot(data["Glucose"])


data.head()


sns.boxplot(data['SkinThickness'])





# create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data.items():
    col_dist = sns.boxplot(value, ax=ax[index])
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)


data.head()


plt.figure(figsize = (16,12))
sns.countplot(data = data, x="Outcome")


count = data["Outcome"].value_counts()
print(count)








data.columns


plt.figure(figsize = (18,12))
plt.scatter(data["Pregnancies"],data["BMI"])


data["Pregnancies"].unique()


plt.figure(figsize = (16,12))
sns.countplot(data=data, x = "Pregnancies")


data.head()


#### Some Distribution plot
sns.distplot(data['Age'].dropna(),kde=False,color='darkred',bins=40)


data.groupby(data["BloodPressure"])["Outcome"].sum()


y = data["Outcome"].copy()
y


X = data.drop("Outcome",axis=1)
X





plt.figure(figsize = (18,12))
plt.hist(data["Age"])





X_train, X_test , y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state = 42)


scale = StandardScaler()
X_train = scale.fit_transform(X_train)
X_test = scale.fit_transform(X_test)





lr = LogisticRegression()
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
lr_pred


cm = confusion_matrix(y_test, lr_pred, labels=lr.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)
disp.plot()


print(classification_report(y_test, lr_pred))


rms = mean_squared_error(y_test, lr_pred, squared=False)
print(rms)



#printing the accuracy for test set
print('Accuracy of Logistic regression model is {}'.format(accuracy_score(y_test,lr_pred)*100))





## Decision Tree model


dt = DecisionTreeClassifier(criterion='entropy',max_depth=2)
dt.fit(X_train, y_train)
dp = dt.predict(X_test)
print(classification_report(y_test,dp))








#plot_confusion_matrix(dt, X_test, y_test)


cm = confusion_matrix(y_test, y_pred=dp, labels=dt.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt.classes_)
disp.plot()


#printing the accuracy for test set
print('Accuracy of Decision Tree model is {}'.format(accuracy_score(y_test,dp)*100))


rms = mean_squared_error(y_test, dp, squared=False)
print(rms)





## Randomforest



rf = RandomForestClassifier().fit(X_train,y_train)
rp = rf.predict(X_test)


print(classification_report(y_test, rp))


cm = confusion_matrix(y_test, y_pred=rp, labels=rf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)
disp.plot()


#printing the accuracy for test set
print('Accuracy of Random forest model is {}'.format(accuracy_score(y_test,rp)*100))


rms = mean_squared_error(y_test, rp, squared=False)
print(rms)








filename = 'diabetes.sav'

joblib.dump(rf, filename)



