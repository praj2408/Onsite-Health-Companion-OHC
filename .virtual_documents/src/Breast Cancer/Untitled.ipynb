


#import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report


# load the dataset
data = pd.read_csv("data.csv")
data.head()





# information about the data
data.info()


shapes = data.shape
print("The dataset has {} rows and {} columns ".format(shapes[0],shapes[1]))


#### See the statistical relationships between the data
data.describe().T





### let's remove the column id and Unnamed:32 
data = data.drop(['id','Unnamed: 32'],axis=1)
data.head()


#### check the null value
null_vals = data.isna().sum()
null_vals


plt.figure(figsize = (18,10))
sns.heatmap(data.isna(), cmap = 'BrBG')





#### Label encoding
label_encoder = preprocessing.LabelEncoder()
data['diagnosis'] = label_encoder.fit_transform(data['diagnosis'])
data.head()








# let's plot the correlation matrix
correlation = data.corr()
plt.figure(figsize = (20,14))
sns.heatmap(correlation, cmap='BrBG', annot = True)


corr_diag = data.corr()['diagnosis']
print(corr_diag)


## correlation greater than 5
corrgt5 = correlation[abs(correlation['diagnosis']) > 0.5]
corrgt5 = corrgt5.index
print(corrgt5)
print()
print("Highly correlated features ",len(corrgt5))



## correlation greater than 5
corrlt5 = correlation[abs(correlation['diagnosis']) <= 0.5]
corrlt5 = corrlt5.index
print(corrlt5)
print()
print("Least correlated features ",len(corrlt5))


#selecting 20 most essential features
data = data[['diagnosis', 'radius_mean', 'area_mean',
       'compactness_mean', 'concavity_mean', 'concave points_mean',
       'perimeter_worst', 'area_worst', 'compactness_worst',
       'concavity_worst', 'concave points_worst','texture_mean', 
         'smoothness_mean', 'symmetry_mean','area_se','fractal_dimension_se',
         'texture_worst', 'smoothness_worst', 'symmetry_worst',
       'fractal_dimension_worst']]
data.head()





#Total number of Malignant and Benign in diagnosis column
data['diagnosis'].value_counts()


plt.figure(figsize = (18,10))
sns.displot(data['diagnosis'])


data.head()


# create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=5, nrows=4, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green',kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)


# selecting only the skewed columns to transform it in gaussian distribution
data_temp = data[['radius_mean', 'area_mean',
       'compactness_mean', 'concavity_mean', 'concave points_mean',
       'area_worst', 'compactness_worst',
       'concavity_worst', 'area_se','fractal_dimension_se',
         'symmetry_worst', 'fractal_dimension_worst']].copy()
print(data_temp.shape)
data_temp.head()


# create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=4, nrows=3, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data_temp.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green',kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)



for feature in data_temp:
    upper_limit = data_temp[feature].mean() + 3*data_temp[feature].std()
    lower_limit = data_temp[feature].mean() - 3*data_temp[feature].std()
    data_temp[feature] = np.where(data_temp[feature]>upper_limit, upper_limit,
                    np.where( data_temp[feature]<lower_limit, lower_limit,
                    data_temp[feature]))


fig, ax = plt.subplots(ncols=4, nrows=3, figsize=(40,30))
index = 0
ax = ax.flatten()

for col, value in data_temp.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green',kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col,fontsize=18)
    col_dist.set_ylabel('density',fontsize=18)
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)


y = data['diagnosis'].copy()



scaler = StandardScaler()
data_temp = scaler.fit_transform(data_temp)


X_train, X_test, y_train, y_test= train_test_split(data_temp, y, test_size = 0.2, random_state=42)









log = LogisticRegression()
log.fit(X_train, y_train)
X_test = scaler.fit_transform(X_test)
pred = log.predict(X_test)


classification_report(y_test,pred)


print('Accuracy of Logistic Regression model is {}'.format(accuracy_score(y_test,pred)*100))







































